### run_analysis.R README
=========================

This is a brief summary of the files that went into putting together the tidy set of data that was submitted for the course project for Getting And Cleaning Data.

Author:  John Bejarano

=====================
## What is this data?
=====================

This is data that was summarized from a study of signals generated by wearable electronic devices.  In this study thirty subjects were instructed to wear a Samsung Galaxy S smartphone while performing routine human activities (e.g. standing, walking, walking upstairs, etc.).  The smartphones worn by the subjects are equipped with a three-axis accelerometer, and a three-axis gyroscope that each produce a stream of numeric signals in each of their axes.  The researchers instructed the subjects to perform one of six activities which they manually recorded, and captured signal output from the smartphone's motion dectetors.

For this particular data set, I took means and standard deviations from several measurements as described below, and calculated means of these means and standard deviations for each combination of subject, activity, and measurement.  Thus, each row of this data represents one subject performing a particular activity, and each column (other than those that identify the subject and activity) represents a mean value calculated from one of the original mean or standard deviation measurements.  While typically means of means and standard deviations without proper weighting are of dubious mathematical value, that is the analysis called for in this exercise.

Also of note, the raw data as presented to me was originally split into a training data set and a test data set with different subjects allocated to each set.  The purpose of this exercise is not to perform machine learning (as would be facilitated by the splitting of this data into training and test sets), but is rather to gather all available data together and summarize it.  Thus, the subjects from the original training and test data sets have been combined into a single data set.

The study was originally performed and presented by:
Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine. International Workshop of Ambient Assisted Living (IWAAL 2012). Vitoria-Gasteiz, Spain. Dec 2012


The data was made available to me and my classmates from the University of California, Irvine.

For additional information, please see the README.txt file that was provided by UC Irvine in the raw data set folder [here](http://archive.ics.uci.edu/ml/machine-learning-databases/00240/).  The raw data that I used for this analysis is also available in that location.

=================
## Raw data files
=================

The UCI Human Activity Recognition data was provided to me in a zip file.  While I downloaded the data from the course web-site, the data is available in the raw data set folder linked above.

To access the data I extracted the file (in Windows 7 on my machine).  The extraction process creates a directory called "UCI HAR Dataset".  This directory can be considered the "root" directory of the data, and should be your working directory if you are to run the run_analysis.R script to perform this analysis.  (I should note that the script actually has a commented line that explicitly sets the working directory to where it should be on my machine.  You may uncomment this line and change the path of the working directory to one applicable to you at your convenience.)

Within this "UCI HAR Dataset" directory, you will find two subdirectories, and four text files as follows.  The text files are as follows:

* activity_labels.txt:  This is a data file containing natural language descriptors for each of the six human activities that are encoded in the main data sets.
* features.txt:  This is a data file that contains the names of each of the 561 variables that are measured for each observation.
* features_info.txt:  This is an informational file (not for consumption by the R script) that describes what the variables are and their naming strategy.  While the codebook submitted with this project describes the tidy data set that is the centerpiece of this project.  This features_info.txt file is a codebook of sorts for the raw data.
* README.txt:  This is an informational file that provides context for what the data set is, how it was produced as how it is structred.  I recommend consulting that file as well when interpreting the analysis submitted in this project.

The two subdirectories are "test" and "train".  As noted above, the original data was pre-split into training data and test data by subject to facilitate machine learning exercises.  These directories contain files as follows:

* test/subject_test.txt:  This is a data file indicating which subject was involved in each of the 2,947 observations of data that was split off into the test data set.
* test/X_test.txt:  This is a data file containing all values for each of the 561 variables listed in "features.txt" for each of the 2,947 observations in the test data set.
* test/y_test.txt:  This is a data file containing the manually recorded human activity that the subject was engaged in during each of the 2,947 observations in the test data set.  These activities are encoded as integers that are enumerated in "activity_labels.txt".
* test/Inertial Signals:  This is an additional sub-directory containing data that is not pertinent to this analysis.

* train/subject_train.txt:  This is a data file indicating which subject was involved in each of the 7,352 observations of data that was split off into the training data set.  Note that since the test and training data were split by subject, no subject_id's listed in this file will appear in the test data's version of this file and vice versa.
* train/X_train.txt:  This is a data file containing all values for each of the 561 variables listed in "features.txt" for each of the 7,352 observations in the training data set.
* train/y_train.txt:  This is a data file containing the manually recorded human activity that the subject was engaged in during each of the 7,352 observations in the training data set.  These activities are encoded as integers that are enumerated in "activity_labels.txt".
* train/Inertial Signals:  This is an additional sub-directory containing data that is not pertinent to this analysis.

Note that the "X" in the above file names is capitalized, while the "y" is lower case.

==================
## Data processing
==================

1.  The run_analysis.R script begins by loading the "dplyr" library as it will use that library for some transformations of data.  If you have not already done so, please ensure that you have downloaded the "dplyr" package from a CRAN mirror for use with this script.
1.  The next activity is to read in all of the raw data sets and to give their columns meaningful names.  All files are read in using read.table with all of the default parameters.  (All column names referenced below are without the quotes.  This is done in the following order:
    - Starting in the "root" directory of the UCI HAR data, the first is "activity_labels.txt".  This is loaded into a data frame called df_activity, and the names of its columns are changed to "activity_id" and "activity_name".
    - Next is "features.txt".  This is loaded into a data frame called df_feature, and the names of its columns are changed to "feature_id" and "feature_name".
    - Moving to the "test" directory, the next file loaded is "subject_test.txt".  This is loaded into a data frame called df_subject_test and its only column's name is changed to "subject_id".
    - Next is "X_test.txt".  This is loaded into a data frame called df_x_test (with a lower case x to avoid future typos).  There are 561 columns in this data frame and they are the same as those features listed in df_feature.  Thus, I rename all the columns in this data set to df_feature$feature_name.
    - The last file in this directory to be loaded is "y_test.txt".  This is loaded into a data frame called df_y_test and its only column's name is changed to "activity_id".
    - Moving to the "train" directory (which is also one directory below the "root" UCI HAR data directory), the next file loaded is "subject_train.txt".  This is loaded into a data frame called df_subject_train and its only column's name is changed to "subject_id".
    - Next is "X_train.txt".  This is loaded into a data frame called df_x_train (again with the lower case x).  Similar to df_x_test, it's columns' names are set to df_feature$feature_name.  (Note that while it may be unnecessary to rename the second data frame's column names as they will ultimately be combined into one, it is helpful while exploring the data in each data frame.)
    - The last file in this directory (and the last file overall) to be loaded is "y_train.txt".  This is loaded into a data frame called df_y_test and its only column's name is changed to "activity_id".
1.  Now that all the raw data is loaded into R and given proper column names, the next step is to bind together (using cbind) the test data into a data frame called df_test.  The order that I combine the test data is df_subject_test, then df_x_test, then df_y_test.  In the new combined data set, each column retains its column names.
1.  Then, I do the same thing with the training data, and load it into a data frame called df_train.  Again, I use cbind and first use df_subject_train, then df_x_train, and then df_y_train.  It's important to do both the test and training data in that order so that df_test's and df_train's columns line up with one another for the next step.
1.  With test and training data each stitched together, I combine both data sets using rbind into a dataset called df_all.  This dataset now includes all 10,299 observations from both the test and training data sets.
1.  Next, we need to bring in more human readable activity names.  To accomplish this, I use merge to merge df_all and df_activity by the common column name "activity_id", effectively adding an "activity_name" column to df_all.  To make this new "activity_name" column easy to work with, I perform a small transformation whereby the new "activity_name" column is moved to the third column, just after "subject_id", and "activity_id".
1.  The next step is to cull down the number of columns to only those that represent means and standard deviations in the original data.  I create a new dataset called df_ms ("ms" standing for mean and standard deviation) by using grepl on df_all and only bringing over "subject_id", "activity_name", and any column whose name includes "mean()" or "std()".  There are other columns that include "MeanFreq" which we do not wish to include.  This is why I use the parentheses in the column names to identify the actual columns I want.  The result is a data set with all 10,299 observations of 68 variables, (the 2 identifying variables, and the 66 "signal" variables that came from the smartphones).
1.  Now that I have used the parentheses in the column names to help me cull down the columns to just what I need, I perform a few steps to make the column names more palatable.  All of these steps use the sub function:
    - I remove all of the parentheses from column names to avoid confusion with R function calls.
    - I change all hyphens in column names to underscores to avoid confusion with the subraction operator in R.
    - I change the "t" in front of all time-based column names to "time" to add clarity.  (See the codebook for which variables are "t" variables.)
    - I change the "f" in front of all frequency-based column names to "freq" to add clarity.  (See the codebook for which variables are "f" variables.)
    - Lastly, there was a glitch in the preparation of the original raw data whereby several "Body" based signal variables had the word "Body" twice.  I remove this redundancy.
1.  Finally, to produce the small tidy data set, called df_summary, I use chained dplyr commands as follows:
    - I begin with df_ms.
    - I use group_by to group df_ms by "subject_id" and "activity_name".
    - I then use summarise to provide mean values of all of the remaining "signal" variables.  It is this last step that calculates the mean of the means and standard deviations.  In this last step, I explicitly retain the column names I was using before.
1.  The last step of the run_analysis.R script is to return to the "root" directory of the UCI HAR data, and write out df_summary using the write.table function with row.names set to FALSE.

==========================
## Note about memory usage
==========================

This script assumes an ample amount of memory available for all of the intermediate data sets that lead to the final tidy data set.  If you have limited memory available to you, some economies of memory can be had by adding rm statements to the script to remove intermediate data frames when they are no longer required.  In the submitted script, I have chosen to not remove these intermediate data frames so as to allow each intermediate step to be examined after data processing is completed.